# -*- coding: utf-8 -*-
"""siamesenetwork-faceverification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iYYBFKgyF-bGFpj54oZHELXgZQZUx2zy

# **Face Recognition**

Facial Recognition System is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services, works by pinpointing and measuring facial features from a given image.

We'll be building a face recognition model that uses Siamese Networks to give us a distance value that indicates whether 2 images are same or different.

The Dataset
We'll be using the Extracted Faces from face-recognition-dataset, which is derived from the LFW Dataset. The Extracted Faces contains faces extracted from the base images using Haar-Cascade Face-Detection (CV2).

* The dataset contains 1324 different individuals, with 2-50 images per person.
* The images are of size (128,128,3) and are encoded in RGB.
* Each folder and image is named with a number, i.e 0.jpg, 1.jpg
"""

import os
import cv2
import time
import random
import numpy as np

import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import preprocess_input

import seaborn as sns
import matplotlib.pyplot as plt
tf.__version__, np.__version__

"""## Reading the Dataset
We're reading the folders and splitting them into train and test set for training purposes.
"""

# Setting random seeds to enable consistency while testing.
random.seed(5)
np.random.seed(5)
tf.random.set_seed(5)

ROOT = r"C:\Users\hosam\Downloads\TAs\CV\AI dept - updated materials\Lab5\Data\Extracted Faces\Extracted Faces"

def read_image(index):
    path = os.path.join(ROOT, index[0], index[1])
    image = cv2.imread(path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return image

def split_dataset(directory, split=0.9):
    folders = os.listdir(directory)
    num_train = int(len(folders)*split)
    
    random.shuffle(folders)
    
    train_list, test_list = {}, {}
    
    # Creating Train-list
    for folder in folders[:num_train]:
        num_files = len(os.listdir(os.path.join(directory, folder)))
        train_list[folder] = num_files
    
    # Creating Test-list
    for folder in folders[num_train:]:
        num_files = len(os.listdir(os.path.join(directory, folder)))
        test_list[folder] = num_files  
    
    return train_list, test_list

train_list, test_list = split_dataset(ROOT, split=0.9)
print("Length of training list:", len(train_list))
print("Length of testing list :", len(test_list))

# train_list, test list contains the folder names along with the number of files in the folder.
print("\nTest List:", test_list)

"""## Creating Triplets
We use the train and test list to create triplets of (anchor, postive, negative) face data, where positive is the same person and negative is a different person than anchor.
"""

def create_triplets(directory, folder_list, max_files=10):
    triplets = []
    folders = list(folder_list.keys())
    
    for folder in folders:
        path = os.path.join(directory, folder)
        files = list(os.listdir(path))[:max_files]
        num_files = len(files)
        
        for i in range(num_files-1):
            for j in range(i+1, num_files):
                anchor = (folder, f"{i}.jpg")
                positive = (folder, f"{j}.jpg")

                neg_folder = folder
                while neg_folder == folder:
                    neg_folder = random.choice(folders)
                neg_file = random.randint(0, folder_list[neg_folder]-1)
                negative = (neg_folder, f"{neg_file}.jpg")

                triplets.append((anchor, positive, negative))
            
    random.shuffle(triplets)
    return triplets

train_triplet = create_triplets(ROOT, train_list)
test_triplet  = create_triplets(ROOT, test_list)

print("Number of training triplets:", len(train_triplet))
print("Number of testing triplets :", len(test_triplet))

print("\nExamples of triplets:")
for i in range(5):
    print(train_triplet[i])

"""## Creating Batch-GeneratorÂ¶
Creating a Batch-Generator that converts the triplets passed into batches of face-data and preproccesses it before returning the data into seperate lists.

Parameters:

* Batch_size: Batch_size of the data to return
* Preprocess: Whether to preprocess the data or not
"""

def get_batch(triplet_list, batch_size=256, preprocess=True):
    batch_steps = len(triplet_list)//batch_size
    
    for i in range(batch_steps+1):
        anchor   = []
        positive = []
        negative = []
        
        j = i*batch_size
        while j<(i+1)*batch_size and j<len(triplet_list):
            a, p, n = triplet_list[j]
            anchor.append(read_image(a))
            positive.append(read_image(p))
            negative.append(read_image(n))
            j+=1
            
        anchor = np.array(anchor)
        positive = np.array(positive)
        negative = np.array(negative)
        
        if preprocess:
            anchor = preprocess_input(anchor)
            positive = preprocess_input(positive)
            negative = preprocess_input(negative)
        
        yield ([anchor, positive, negative])

"""## Plotting the Data
Plotting the data generated from get_batch() to see the results
"""

num_plots = 6

f, axes = plt.subplots(num_plots, 3, figsize=(15, 20))

for x in get_batch(train_triplet, batch_size=num_plots, preprocess=False):
    a,p,n = x
    for i in range(num_plots):
        axes[i, 0].imshow(a[i])
        axes[i, 1].imshow(p[i])
        axes[i, 2].imshow(n[i])
        i+=1
    break

"""## Creating the Model
Unlike a conventional CNN, the Siamese Network does not classify the images into certain categories or labels, rather it only finds out the distance between any two given images. If the images have the same label, then the network should learn the parameters, i.e. the weights and the biases in such a way that it should produce a smaller distance between the two images, and if they belong to different labels, then the distance should be larger

Siamese Network Image
![](https://miro.medium.com/max/2000/1*05hUCDHhnl4hdjqvdVTHtw.png)
"""

from tensorflow.keras import backend, layers, metrics

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import Xception
from tensorflow.keras.models import Model, Sequential

from tensorflow.keras.utils import plot_model
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""## Encoder
The Encoder is responsible for converting the passed images into their feature vectors. We're using a pretrained model, Xception model which is based on Inception_V3 model. By using transfer learning, we can significantly reduce the training time and size of the dataset.

The Model is connected to Fully Connected (Dense) layers and the last layer normalises the data using L2 Normalisation. (L2 Normalisation is a technique that modifies the dataset values in a way that in each row the sum of the squares will always be up to 1)
"""

def get_encoder(input_shape):
    """ Returns the image encoding model """

    pretrained_model = Xception(
        input_shape=input_shape,
        weights='imagenet',
        include_top=False,
        pooling='avg',
    )
    
    for i in range(len(pretrained_model.layers)-27):
        pretrained_model.layers[i].trainable = False

    encode_model = Sequential([
        pretrained_model,
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(256, activation="relu"),
        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))
    ], name="Encode_Model")
    return encode_model

"""Siamese Network
We're creating a Siamese Network that takes 3 input images, (anchor, postive, negative) and uses the encoder above to encode the images to their feature vectors. Those features are passed to a distance layer which computes the distance between (anchor, positive) and (anchor, negative) pairs.

We'll be defining a custom layer to compute the distance.

Distance Formula:

![](data:image/png;base64,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)
"""

class DistanceLayer(layers.Layer):
    # A layer to compute âf(A) - f(P)âÂ² and âf(A) - f(N)âÂ²
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, anchor, positive, negative):
        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)
        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)
        return (ap_distance, an_distance)
    

def get_siamese_network(input_shape = (128, 128, 3)):
    encoder = get_encoder(input_shape)
    
    # Input Layers for the images
    anchor_input   = layers.Input(input_shape, name="Anchor_Input")
    positive_input = layers.Input(input_shape, name="Positive_Input")
    negative_input = layers.Input(input_shape, name="Negative_Input")
    
    ## Generate the encodings (feature vectors) for the images
    encoded_a = encoder(anchor_input)
    encoded_p = encoder(positive_input)
    encoded_n = encoder(negative_input)
    
    # A layer to compute âf(A) - f(P)âÂ² and âf(A) - f(N)âÂ²
    distances = DistanceLayer()(
        encoder(anchor_input),
        encoder(positive_input),
        encoder(negative_input)
    )
    
    # Creating the Model
    siamese_network = Model(
        inputs  = [anchor_input, positive_input, negative_input],
        outputs = distances,
        name = "Siamese_Network"
    )
    return siamese_network

siamese_network = get_siamese_network()
siamese_network.summary()

plot_model(siamese_network, show_shapes=True, show_layer_names=True)

"""## Putting everything together
We now need to implement a model with custom training loop and loss function so we can compute the triplet loss using the three embeddings produced by the Siamese network.

We'll create a Mean metric instance to track the loss of the training process.

Triplet Loss Function:

### ![](https://miro.medium.com/max/1838/0*AX2TSZNk19_gDgTN.png)
"""

class SiameseModel(Model):
    # Builds a Siamese model based on a base-model
    def __init__(self, siamese_network, margin=1.0):
        super(SiameseModel, self).__init__()
        
        self.margin = margin
        self.siamese_network = siamese_network
        self.loss_tracker = metrics.Mean(name="loss")

    def call(self, inputs):
        return self.siamese_network(inputs)

    def train_step(self, data):
        # GradientTape get the gradients when we compute loss, and uses them to update the weights
        with tf.GradientTape() as tape:
            loss = self._compute_loss(data)
            
        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)
        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))
        
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

    def test_step(self, data):
        loss = self._compute_loss(data)
        
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

    def _compute_loss(self, data):
        # Get the two distances from the network, then compute the triplet loss
        ap_distance, an_distance = self.siamese_network(data)
        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)
        return loss

    @property
    def metrics(self):
        # We need to list our metrics so the reset_states() can be called automatically.
        return [self.loss_tracker]

siamese_model = SiameseModel(siamese_network)
optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)
siamese_model.compile(optimizer=optimizer)

"""## Training the Model
We'll now be training the siamese_model on batches of triplets. We'll print the training loss, along with additional metrics from testing every epoch. The model weights will also be saved whenever it outperforms the previous max_accuracy.

We're hoping to collect more metrics about the model to evaluate how to increase the accuracy of the model. The epochs have been set to avoid going over Kaggle's time constraint.

### Test Function
test_on_triplets() function will be responsible for testing the model on test_triplets. It'll collect metrics (accuracy, means, stds) by predicting on the train data. We'll also be printing the Accuracy of the model after testing.
"""

def test_on_triplets(batch_size = 256):
    pos_scores, neg_scores = [], []

    for data in get_batch(test_triplet, batch_size=batch_size):
        prediction = siamese_model.predict(data)
        pos_scores += list(prediction[0])
        neg_scores += list(prediction[1])
    
    accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) / len(pos_scores)
    ap_mean = np.mean(pos_scores)
    an_mean = np.mean(neg_scores)
    ap_stds = np.std(pos_scores)
    an_stds = np.std(neg_scores)
    
    print(f"Accuracy on test = {accuracy:.5f}")
    return (accuracy, ap_mean, an_mean, ap_stds, an_stds)

save_all = False
epochs = 30
batch_size = 128

max_acc = 0
train_loss = []
test_metrics = []

for epoch in range(1, epochs+1):
    t = time.time()
    
    # Training the model on train data
    epoch_loss = []
    for data in get_batch(train_triplet, batch_size=batch_size):
        loss = siamese_model.train_on_batch(data)
        epoch_loss.append(loss)
    epoch_loss = sum(epoch_loss)/len(epoch_loss)
    train_loss.append(epoch_loss)

    print(f"\nEPOCH: {epoch} \t (Epoch done in {int(time.time()-t)} sec)")
    print(f"Loss on train    = {epoch_loss:.5f}")
    
    # Testing the model on test data
    metric = test_on_triplets(batch_size=batch_size)
    test_metrics.append(metric)
    accuracy = metric[0]
    
    # Saving the model weights
    if save_all or accuracy>=max_acc:
        siamese_model.save_weights("siamese_model")
        max_acc = accuracy

# Saving the model after all epochs run
siamese_model.save_weights("siamese_model-final")

"""## Using the Model
Now that we've finished training our model, we need to extract the encoder so that we can use it to encode images and then get use the feature vectors to compute the distance between those images.

We'll also be saving the encoder for later use.
"""

def extract_encoder(model):
    encoder = get_encoder((128, 128, 3))
    i=0
    for e_layer in model.layers[0].layers[3].layers:
        layer_weight = e_layer.get_weights()
        encoder.layers[i].set_weights(layer_weight)
        i+=1
    return encoder

encoder = extract_encoder(siamese_model)
encoder.save_weights("encoder")
encoder.summary()

"""## Classify Images
To compute the distance between the encodings of the images, we'll be using distance formula. Distance over a certain threshold to be "different" and below the threshold as "same".
"""

def classify_images(face_list1, face_list2, threshold=1.3):
    # Getting the encodings for the passed faces
    tensor1 = encoder.predict(face_list1)
    tensor2 = encoder.predict(face_list2)
    
    distance = np.sum(np.square(tensor1-tensor2), axis=-1)
    prediction = np.where(distance<=threshold, 0, 1)
    return prediction

def ModelMetrics(pos_list, neg_list):
    true = np.array([0]*len(pos_list)+[1]*len(neg_list))
    pred = np.append(pos_list, neg_list)
    
    # Compute and print the accuracy
    print(f"\nAccuracy of model: {accuracy_score(true, pred)}\n")
    
    # Compute and plot the Confusion matrix
    cf_matrix = confusion_matrix(true, pred)

    categories  = ['Similar','Different']
    names = ['True Similar','False Similar', 'False Different','True Different']
    percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]

    labels = [f'{v1}\n{v2}' for v1, v2 in zip(names, percentages)]
    labels = np.asarray(labels).reshape(2,2)

    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',
                xticklabels = categories, yticklabels = categories)

    plt.xlabel("Predicted", fontdict = {'size':14}, labelpad = 10)
    plt.ylabel("Actual"   , fontdict = {'size':14}, labelpad = 10)
    plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)


pos_list = np.array([])
neg_list = np.array([])

for data in get_batch(test_triplet, batch_size=256):
    a, p, n = data
    pos_list = np.append(pos_list, classify_images(a, p))
    neg_list = np.append(neg_list, classify_images(a, n))
    break

ModelMetrics(pos_list, neg_list)